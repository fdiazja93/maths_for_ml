{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent\n",
    "\n",
    "This notebook reviews a widely used algorithm in optimization and machine learning called gradient descent. The primary goal of this algorithm is to find the points that numerically minimize a given function.\n",
    "\n",
    "For functions of one or two variables, finding the minimum is typically straightforward. In the 1D case, this involves calculating the derivative of the function and solving a simple algebraic equation. For 2D functions, the process extends to solving a system of two equations involving the gradient. However, as the number of dimensions increases, the number of equations to solve grows as well. This makes analytical methods computationally expensive for high-dimensional problems.\n",
    "\n",
    "In machine learning, the dimensionality of a problem is often tied to the number of features in the dataset under consideration. It is not uncommon for datasets to contain thousands of features. Minimizing the cost function in such cases would require solving a system of thousands of equations, which is computationally expensive. Thus, iterative optimization techniques like gradient descent are used as an efficient alternative.\n",
    "\n",
    "Gradient descent avoids solving these equations directly. Instead, it iteratively updates the parameters by moving in the direction of the negative gradient (i.e., the steepest descent). This approach is particularly effective for high-dimensional problems, making it a cornerstone of modern machine learning optimization. For a more detailed review of the mathematics behind the gradient descent algorithm, look at the notes in the repository (Under construction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "import jax \n",
    "import jax.numpy as jnp\n",
    "import time # This library is included to control the time each frame of the dynamical plot appears"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an implementation of the gradient descent algorithm in 1D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_1D(f, x_min, x_max, N, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Implements the gradient descent algorithm in one dimension to the point x_i where the minimum of the function f lies in the interval [x_min, x_max].\n",
    "\n",
    "    Parameters:\n",
    "    f (function): The objective function of which we wish to find the point where the minimum lies.\n",
    "    x_min (Real): The lower bound of the interval where we look for the minimum.\n",
    "    x_max (Real): The upper bound of the interval where we look for the minimum.\n",
    "    N (int): The number of iterations.\n",
    "    learning_rate (Real): The learning rate which controls the resolution of the steps between points.\n",
    "    \"\"\"\n",
    "    x = np.linspace(x_min, x_max, 100) # Definition of the interval/x-axis.\n",
    "    x_0 = np.random.uniform(x_min, x_max) # Random choice of initial point.\n",
    "    x_i = x_0\n",
    "    fig, ax = plt.subplots()\n",
    "    gradient = jax.grad(f) # Gradient computation.\n",
    "\n",
    "    for i in range(N):\n",
    "        x_i = x_i - learning_rate * gradient(x_i) # Point update.\n",
    "        x_i = np.clip(x_i, x_min, x_max) # Ensure that x_i lies within the interval [x_min, x_max]. If x_i < x_min, x_i is set to x_min. If x_i > x_max, x_i is set to x_max.\n",
    "        \n",
    "        # Clear and redraw plot\n",
    "        ax.cla()\n",
    "        plt.grid()\n",
    "        ax.plot(x, f(x), label=r'$f(x)$')\n",
    "        ax.plot([x_i], [f(x_i)], marker='o', color='red', label=r'$f(x_i)$')\n",
    "        ax.legend(loc='upper right')\n",
    "        ax.set_title(f\"Iteration {i+1}/{N}\")\n",
    "        plt.xlabel(r'$x$')\n",
    "        plt.ylabel(r'$f(x)$')\n",
    "        clear_output(wait=True)  # Clear previous output\n",
    "        display(fig)            # Display updated figure\n",
    "        time.sleep(0.3)\n",
    "\n",
    "        # Stop iterating when we are very close to minimum.\n",
    "        if abs(gradient(x_i)) < 1e-6:\n",
    "            print(f\"Converged at iteration {i+1}\")\n",
    "            break\n",
    "        \n",
    "    return x_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x):\n",
    "    return (3 - x ** 2) ** 2 # This function is important in physics. It is the Higgs or double well potenital.\n",
    "\n",
    "gradient_descent_1D(f1, -3., 3., 100, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the implementation of the gradient descent algorithm in 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_2D(f, vec_x_min, vec_x_max, N, learning_rate=0.01, seed=None):\n",
    "    \"\"\"\n",
    "    Implements gradient descent in 2D for minimizing a function f with 3D visualization.\n",
    "    \n",
    "    Parameters:\n",
    "        f (function): The objective function to minimize, taking two arguments.\n",
    "        vec_x_min (list): Lower bounds for the search space [x_min, y_min].\n",
    "        vec_x_max (list): Upper bounds for the search space [x_max, y_max].\n",
    "        N (int): Number of iterations.\n",
    "        learning_rate (float): Step size for updating the parameters.\n",
    "    \n",
    "    Returns:\n",
    "        vec_x_i (list): Approximate coordinates of the minimum point.\n",
    "        f_value (float): Function value at the minimum point.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Random initialization within bounds\n",
    "    vec_x_0 = [np.random.uniform(vec_x_min[0], vec_x_max[0]), np.random.uniform(vec_x_min[1], vec_x_max[1])] # Random choice of initial point.\n",
    "    vec_x_i = np.array(vec_x_0)\n",
    "    \n",
    "    nablaf = [jax.grad(f, argnums=i) for i in range(2)] # Gradient computation.\n",
    "    \n",
    "    # Mesh grid for the surface plot.\n",
    "    x = np.linspace(vec_x_min[0], vec_x_max[0], 100)\n",
    "    y = np.linspace(vec_x_min[1], vec_x_max[1], 100)\n",
    "    X, Y = np.meshgrid(x, y) # Grid wih one x-axis for each point in y.\n",
    "    Z = np.array(f(X, Y)) # Evaluate function on the grid.\n",
    "    \n",
    "    # Set up the 3D plot.\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Gradient descent loop\n",
    "    for i in range(N):\n",
    "        \n",
    "        gradients = np.array([nablaf[k](vec_x_i[0], vec_x_i[1]) for k in range(2)])\n",
    "        vec_x_i = vec_x_i - learning_rate * gradients # Update point.\n",
    "        \n",
    "        vec_x_i = np.clip(vec_x_i, vec_x_min, vec_x_max) # Ensure that point lies within desired interval.\n",
    "        \n",
    "        # Clear and update plot.\n",
    "        ax.cla()\n",
    "        ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8, edgecolor='none')  # Plot surface\n",
    "        ax.scatter(vec_x_i[0], vec_x_i[1], f(vec_x_i[0], vec_x_i[1]), color='red', s=50, label=f'Iteration {i+1}/{N}')\n",
    "        \n",
    "        ax.set_title(f\"Gradient Descent Iteration {i+1}/{N}\")\n",
    "        ax.set_xlabel('X-axis')\n",
    "        ax.set_ylabel('Y-axis')\n",
    "        ax.set_zlabel('Z-axis')\n",
    "        ax.legend(loc='upper left')\n",
    "        \n",
    "        # Update display\n",
    "        clear_output(wait=True)\n",
    "        display(fig)\n",
    "        time.sleep(0.3)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return vec_x_i, f(vec_x_i[0], vec_x_i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F(x, y):\n",
    "    return x ** 2 + y ** 2\n",
    "\n",
    "gradient_descent_2D(F, [-2., -2.], [2., 2.], 100, 0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
